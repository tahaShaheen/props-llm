{{ episode_reward_buffer_string }}

Above are all previous params attempts from the buffer, showing for each params set:
- the actual episodic reward f(params)
- the reward guess you previously predicted
- the actual human feedback (if collected)
- the feedback guess you previously predicted (only for feedback-checkpoint episodes)
- and trajectories in the form of (state, action), (state, action), ... until the episode ends.

IMPORTANT: The predicted_reward and predicted_feedback shown above were generated by YOU in earlier iterations.
Use the mismatch between your past predictions and actual outcomes to calibrate your future predictions.

**IMPORTANT: Feedback fields in the buffer**
For each prior params set, feedback appears with explicit tags:
- `HUMAN FEEDBACK`: this is what a human observer actually said after watching that policy.
    If no human feedback was collected, it will be shown as `N/A`.
- `LLM PREDICTED FEEDBACK`: this is what YOU predicted the human would say in earlier feedback-checkpoint iterations.
    If no predicted feedback was available, it will be shown as `N/A`.

Do not confuse these:
- `HUMAN FEEDBACK` is ground truth from a human.
- `LLM PREDICTED FEEDBACK` is your prior guess.

You are a good global RL policy optimizer. Use the differences in predicted and actual reward and predicted and actual feedback (if provided) to help me find the global optimal policy in the following environment:

You have two objectives at the same time:
1) Optimize the policy to increase actual reward.
{% if request_feedback_prediction %}
2) Optimize the policy so your predicted_reward and predicted_feedback better match actual reward and actual feedback.
{% else %}
2) Optimize the policy so your predicted_reward better matches actual reward.
{% endif %}

# Environment:
{% include env_description %}

# Regarding the policy parameters **params**:
**params** is an array of {{ rank }} float numbers.
**params** values are in the range of [-6.0, 6.0] with 1 decimal place.
params represent a linear policy.
f(params) is the episodic reward of the policy.

# Here's how we'll interact:
1. I will first provide MAX_ITERATIONS ({{ num_episodes }}) along with a few training examples.
{% if request_feedback_prediction %}
2. You will provide your response in the following exact 4-line format ONLY:
    * 'explanation: E'
    * 'params[0]: X, params[1]: Y, params[2]: Z,..., params[{{ rank - 1 }}]: W'
    * 'predicted_reward: R'
    * 'predicted_feedback: F'
    Where:
    - E is a short explanation of which params you are changing and why.
    - R is your predicted episodic reward for the params you propose.
        - F is your forecast of what the human will likely say after observing the params you propose.
            It is NOT the current human feedback text and should not just copy `HUMAN FEEDBACK`.
    - params values are in the range of [-6.0, 6.0], with 1 decimal place.
    OUTPUT ONLY THESE 4 LINES, NOTHING ELSE.
{% else %}
2. You will provide your response in the following exact 3-line format ONLY:
    * 'explanation: E'
    * 'params[0]: X, params[1]: Y, params[2]: Z,..., params[{{ rank - 1 }}]: W'
    * 'predicted_reward: R'
    Where:
    - E is a short explanation of which params you are changing and why.
    - R is your predicted episodic reward for the params you propose.
    - params values are in the range of [-6.0, 6.0], with 1 decimal place.
    OUTPUT ONLY THESE 3 LINES, NOTHING ELSE.
{% endif %}
3. I will then provide the policy's obtained reward f(params) averaged over {{ num_evaluation_episodes }} episodes, given the last iteration's params.
4. A human observer will watch the policy execute and may provide feedback on the behavior.
5. We will repeat steps 2-4 until we reach the maximum number of iterations.

# Remember:
1. **Do not propose previously seen params.**
2. **The global optimal reward should be around {{ optimum }}.** If you are below that, this is just a local optimum. You should explore instead of exploiting.
3. Search both positive and negative values. **During exploration, use search step size of {{ step_size }}**.
{% if request_feedback_prediction %}
4. Your predicted_reward and predicted_feedback should be your best honest forecast for the params you propose, based on the history above.
5. Treat `predicted_feedback` as a prediction target: try to make it match future `HUMAN FEEDBACK` better over time.
{% else %}
4. Your predicted_reward should be your best honest forecast for the params you propose, based on the history above.
{% endif %}

# HUMAN FEEDBACK:
You may receive feedback from a human observer who watched the policy execute in the environment.
**Take this feedback seriously** - the human can see qualitative aspects of the behavior that the reward number alone cannot capture.
Use the feedback to guide your next parameter choice. For example:
- If the human says the agent is "falling over", try to stabilize the policy.
- If the human says the agent is "moving too slowly", try to increase action magnitudes.
- If the human says the policy is "good but shaky", make small refinements.

{% if human_feedback and last_policy_params %}
# PARAMS THAT RECEIVED HUMAN FEEDBACK (indexed format):
{{ last_policy_params }}
{% endif %}

{% if human_feedback %}
A human observer watched this policy execute and provided this feedback:
"{{ human_feedback }}"

**Consider this feedback carefully when proposing your next params.**
{% endif %}

Now you are at iteration {{step_number}} out of {{ num_episodes }}. 
{% if request_feedback_prediction %}
CRITICAL: Output exactly these 4 lines and nothing else:
1) explanation: E
2) params[0]: X, params[1]: Y, params[2]: Z,..., params[{{ rank - 1 }}]: W
3) predicted_reward: R
4) predicted_feedback: F
{% else %}
CRITICAL: Output exactly these 3 lines and nothing else:
1) explanation: E
2) params[0]: X, params[1]: Y, params[2]: Z,..., params[{{ rank - 1 }}]: W
3) predicted_reward: R
{% endif %}

You WILL be stopped if you spend too much time thinking. BE FAST!

{% if attempt_idx > 0 %}
{% if request_feedback_prediction %}
WARNING: Attempt {{attempt_idx + 1}}/10. FAILED {{attempt_idx}} times. {{10 - attempt_idx - 1}} remaining. OUTPUT MUST BE EXACTLY 4 LINES: explanation: E / params[0]: X, ..., params[{{ rank - 1 }}]: Z / predicted_reward: R / predicted_feedback: F
{% else %}
WARNING: Attempt {{attempt_idx + 1}}/10. FAILED {{attempt_idx}} times. {{10 - attempt_idx - 1}} remaining. OUTPUT MUST BE EXACTLY 3 LINES: explanation: E / params[0]: X, ..., params[{{ rank - 1 }}]: Z / predicted_reward: R
{% endif %}
{% endif %}
