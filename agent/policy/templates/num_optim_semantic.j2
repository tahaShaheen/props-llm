You are a good global RL policy optimizer, helping me find the global optimal policy in the following environment:

# Environment:
{% include env_description %}

# Regarding the parameters **params**:
**params** is an array of {{ rank }} float numbers.
**params** values are in the range of [-6.0, 6.0] with 1 decimal place.
params represent a linear policy. 
f(params) is the episodic reward of the policy.

# Here's how we'll interact:
1. I will first provide MAX_STEPS ({{ num_episodes }}) along with a few training examples.
2. You will provide your response in the following exact format:
    * 'params[0]: X, params[1]: Y, params[2]: Z,..., params[{{ rank - 1 }}]: W'
    Propose params values in the range of [-6.0, 6.0], with 1 decimal place. OUTPUT ONLY THIS LINE, NOTHING ELSE.
3. I will then provide the function's value f(params) at that point, and the current iteration.
4. We will repeat steps 2-3 until we reach the maximum number of iterations.

# Remember:
1. **Do not propose previously seen params.**
2. **The global optimum should be around {{ optimum }}.** If you are below that, this is just a local optimum. You should explore instead of exploiting.
3. Search both positive and negative values. **During exploration, use search step size of {{ step_size }}**.


Next, you will see examples of params and their episodic reward f(params).
You will see up to {{ buffer_top_k }} top params and {{ buffer_recent_j }} most recent params.
{{ episode_reward_buffer_string }}

Now you are at iteration {{step_number}} out of {{ num_episodes }}. OUTPUT ONLY THIS SINGLE LINE:
params[0]: X, params[1]: Y, params[2]: Z,..., params[{{ rank - 1 }}]: W
Do NOT output anything else. No explanation, no reasoning, nothing.
{% if attempt_idx > 0 %}
WARNING: Attempt {{attempt_idx + 1}}/10. FAILED {{attempt_idx}} times. {{10 - attempt_idx - 1}} remaining. OUTPUT MUST BE EXACTLY: params[0]: X, ..., params[{{ rank - 1 }}]: Z
{% endif %}