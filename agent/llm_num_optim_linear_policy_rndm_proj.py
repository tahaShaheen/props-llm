from agent.policy.linear_policy_no_bias import LinearPolicy as LinearPolicyNoBias
from agent.policy.linear_policy import LinearPolicy
from agent.policy.replay_buffer import EpisodeRewardBufferNoBias
from agent.policy.llm_brain_linear_policy import LLMBrain
from world.base_world import BaseWorld
import numpy as np
import re
import time
from utils.console import yellow, green, blue


class LLMNumOptimRndmPrjAgent:
    def __init__(
        self,
        logdir,
        dim_action,
        dim_state,
        max_traj_count,
        max_traj_length,
        llm_si_template,
        llm_output_conversion_template,
        llm_model_name,
        num_evaluation_episodes,
        rank,
        bias,
        optimum,
        ollama_num_ctx=4096,
    ):
        self.start_time = time.process_time()
        self.api_call_time = 0
        self.dim_action = dim_action
        self.dim_state = dim_state
        self.bias = bias
        self.optimum = optimum

        if not self.bias:
            param_count = dim_action * dim_state
        else:
            param_count = dim_action * dim_state + dim_action
        
        self.G = np.random.randn(param_count, param_count)
        self.Q, self.R = np.linalg.qr(self.G)
        self.high_to_low_projection_matrix = self.Q[:, :rank]
        self.low_to_high_projection_matrix = self.Q[:, :rank].T
        self.rank = rank
        
        if not self.bias:
            self.policy = LinearPolicyNoBias(dim_actions=dim_action, dim_states=dim_state)
        else:
            self.policy = LinearPolicy(dim_actions=dim_action, dim_states=dim_state)
        self.replay_buffer = EpisodeRewardBufferNoBias(max_size=max_traj_count)
        self.llm_brain = LLMBrain(
            llm_si_template,
            llm_output_conversion_template,
            llm_model_name,
            ollama_num_ctx=ollama_num_ctx,
        )
        self.logdir = logdir
        self.num_evaluation_episodes = num_evaluation_episodes
        self.training_episodes = 0

        if self.bias:
            self.dim_state += 1
    
    def parameters_high_to_low(self, parameters):
        return parameters.reshape(-1) @ self.high_to_low_projection_matrix
    
    def parameters_low_to_high(self, parameters):
        return (parameters.reshape(-1) @ self.low_to_high_projection_matrix).reshape(self.dim_state, self.dim_action)

    def rollout_episode(self, world: BaseWorld, logging_file, record=True):
        state = world.reset()
        state = np.expand_dims(state, axis=0)
        logging_file.write(f"{', '.join([str(x) for x in self.policy.get_parameters().reshape(-1)])}\n")
        logging_file.write(f"parameter ends\n\n")
        logging_file.write(f"state | action | reward\n")
        done = False
        step_idx = 0
        while not done:
            action = self.policy.get_action(state.T)
            action = np.reshape(action, (1, self.dim_action))
            if world.discretize:
                action = np.argmax(action)
                action = np.array([action])
            next_state, reward, done = world.step(action)
            logging_file.write(f"{state.T[0]} | {action[0]} | {reward}\n")
            state = next_state
            step_idx += 1
        logging_file.write(f"Total reward: {world.get_accu_reward()}\n")
        if record:
            self.replay_buffer.add(
                self.parameters_high_to_low(self.policy.get_parameters()), world.get_accu_reward()
            )
        return world.get_accu_reward()

    def random_warmup(self, world: BaseWorld, logdir, num_episodes):
        for episode in range(num_episodes):
            self.policy.initialize_policy()
            # Run the episode and collect the trajectory
            print(yellow(f"Rolling out warmup episode {episode}..."))
            logging_filename = f"{logdir}/warmup_rollout_{episode}.txt"
            logging_file = open(logging_filename, "w")
            result = self.rollout_episode(world, logging_file)
            print(yellow(f"Result: {result}"))

    def train_policy(self, world: BaseWorld, logdir, search_std):

        def parse_parameters(input_text):
            # Remove <think>...</think> tags first
            import re as regex_module
            cleaned_text = regex_module.sub(r'<think>.*?</think>', '', input_text, flags=regex_module.DOTALL)
            
            # Find all lines that contain params[ pattern
            pattern = regex_module.compile(r'params\[(\d+)\]:\s*([+-]?\d+(?:\.\d+)?)')
            
            # Find the line with the most matches
            best_line = None
            best_matches = []
            
            for line in cleaned_text.split("\n"):
                matches = pattern.findall(line)
                if matches and len(matches) > len(best_matches):
                    best_matches = matches
                    best_line = line
            
            if not best_line:
                raise ValueError(f"Could not find params in output:\n{input_text}")
            
            print(blue(f"response: {best_line}"))

            # Convert matched strings to float
            results = []
            for match in best_matches:
                results.append(float(match[1]))
            print(results)
            assert len(results) == self.rank
            return np.array(results).reshape(-1)

        def str_nd_examples(replay_buffer: EpisodeRewardBufferNoBias, n):

            all_parameters = []
            for weights, reward in replay_buffer.buffer:
                parameters = weights
                all_parameters.append((parameters.reshape(-1), reward))

            text = ""
            for parameters, reward in all_parameters:
                l = ""
                for i in range(n):
                    l += f'params[{i}]: {parameters[i]:.5g}; '
                fxy = reward
                l += f"f(params): {fxy:.2f}\n"
                text += l
            return text


        # Update the policy using llm_brain, q_table and replay_buffer
        print("Updating the policy...")
        new_parameter_list, reasoning, api_time = self.llm_brain.llm_update_parameters_num_optim(
            str_nd_examples(self.replay_buffer, self.rank),
            parse_parameters,
            self.training_episodes,
            search_std,
            self.rank,
            self.optimum,
        )
        self.api_call_time += api_time

        print(self.policy.get_parameters().shape)
        print(new_parameter_list.shape)
        self.policy.update_policy(self.parameters_low_to_high(new_parameter_list))
        print(self.policy.get_parameters().shape)
        logging_q_filename = f"{logdir}/parameters.txt"
        logging_q_file = open(logging_q_filename, "w")
        logging_q_file.write(str(self.policy))
        logging_q_file.close()
        q_reasoning_filename = f"{logdir}/parameters_reasoning.txt"
        q_reasoning_file = open(q_reasoning_filename, "w")
        q_reasoning_file.write(reasoning)
        q_reasoning_file.close()
        print(green("Policy updated!"))


        # Run the episode and collect the trajectory
        print(f"Rolling out episode {self.training_episodes}...")
        logging_filename = f"{logdir}/training_rollout.txt"
        logging_file = open(logging_filename, "w")
        results = []
        for idx in range(20):
            if idx == 0:
                result = self.rollout_episode(world, logging_file, record=False)
            else:
                result = self.rollout_episode(world, logging_file, record=False)
            results.append(result)
        print(f"Results: {results}")
        result = np.mean(results)
        self.replay_buffer.add(new_parameter_list, result)

        self.training_episodes += 1

        _cpu_time = time.process_time() - self.start_time
        _api_time = self.api_call_time
        _total_episodes = self.total_episodes
        _total_steps = self.total_steps
        _total_reward = result
        _parameters = str(new_parameter_list)
        return _cpu_time, _api_time, _total_episodes, _total_steps, _total_reward, _parameters

    def evaluate_policy(self, world: BaseWorld, logdir):
        results = []
        for idx in range(self.num_evaluation_episodes):
            logging_filename = f"{logdir}/evaluation_rollout_{idx}.txt"
            logging_file = open(logging_filename, "w")
            result = self.rollout_episode(world, logging_file, record=False)
            results.append(result)
        return results
