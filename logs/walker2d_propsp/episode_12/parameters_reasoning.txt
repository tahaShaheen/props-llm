You are a good global RL policy optimizer, helping me find the global optimal policy in the following environment:

# Environment:
The walker is a two-dimensional bipedal robot consisting of seven main body parts - a single torso at the top (with the two legs splitting after the torso), two thighs in the middle below the torso, two legs below the thighs, and two feet attached to the legs on which the entire body rests. The goal is to walk in the forward (right) direction by applying torque to the six hinges connecting the seven body parts.

The obverstation space consists of the following 17 parts (in order):
- state[0]: z-coordinate of the torso (height of Walker2d)
- state[1]: angle of the torso
- state[2]: angle of the thigh joint
- state[3]: angle of the leg joint
- state[4]: angle of the foot joint
- state[5]: angle of the left thigh joint
- state[6]: angle of the left leg joint
- state[7]: angle of the left foot joint
- state[8]: velocity of the x-coordinate of the torso
- state[9]: velocity of the z-coordinate (height) of the torso
- state[10]: angular velocity of the angle of the torso
- state[11]: angular velocity of the thigh hinge
- state[12]: angular velocity of the leg hinge
- state[13]: angular velocity of the foot hinge
- state[14]: angular velocity of the thigh hinge
- state[15]: angular velocity of the leg hinge
- state[16]: angular velocity of the foot hinge

The action space consists of the following 6 parts (in order):
- action[0]: Torque applied on the thigh rotor
- action[1]: Torque applied on the leg rotor
- action[2]: Torque applied on the foot rotor
- action[3]: Torque applied on the left thigh rotor
- action[4]: Torque applied on the left leg rotor
- action[5]: Torque applied on the left foot rotor

The policy is a linear policy with 108 parameters and works as follows: 
action = state @ W + B
where 
W = [
     params[0], params[1], params[2], params[3], params[4], params[5],
     params[6], params[7], params[8], params[9], params[10], params[11],
     params[12], params[13], params[14], params[15], params[16], params[17],
     params[18], params[19], params[20], params[21], params[22], params[23],
     params[24], params[25], params[26], params[27], params[28], params[29],
     params[30], params[31], params[32], params[33], params[34], params[35],
     params[36], params[37], params[38], params[39], params[40], params[41],
     params[42], params[43], params[44], params[45], params[46], params[47],
     params[48], params[49], params[50], params[51], params[52], params[53],
     params[54], params[55], params[56], params[57], params[58], params[59],
     params[60], params[61], params[62], params[63], params[64], params[65],
     params[66], params[67], params[68], params[69], params[70], params[71],
     params[72], params[73], params[74], params[75], params[76], params[77],
     params[78], params[79], params[80], params[81], params[82], params[83],
     params[84], params[85], params[86], params[87], params[88], params[89],
     params[90], params[91], params[92], params[93], params[94], params[95],
     params[96], params[97], params[98], params[99], params[100], params[101],
]
B = [
     params[102], params[103], params[104], params[105], params[106], params[107]
]

The total reward is: reward = healthy_reward bonus + forward_reward - ctrl_cost.

healthy_reward: Every timestep that the Walker2d is alive, it receives a fixed reward of value healthy_reward (default is 1).
forward_reward: A reward for moving forward, this reward would be positive if the walker moves forward (in the positive direction / in the right direction). 
ctrl_cost: A negative reward to penalize the Walker2d for taking actions that are too large. 

The goal is to walk in the forward (right) direction by applying torque to the six hinges connecting the seven body parts.

# Regarding the policy parameters **params**:
**params** is an array of 108 float numbers.
**params** values are in the range of [-6.0, 6.0] with 1 decimal place.
params represent a linear policy.
f(params) is the episodic reward of the policy.

# Here's how we'll interact:
1. I will first provide MAX_ITERATIONS (400) along with a few training examples.
2. You will provide your response in the following exact format ONLY:
   * 'params[0]: X, params[1]: Y, params[2]: Z,..., params[107]: W'
   Propose params values in the range of [-6.0, 6.0], with 1 decimal place. OUTPUT ONLY THIS LINE, NOTHING ELSE.
3. I will then provide the policy's obtained reward f(params) averaged over 20 episodes, given the last iteration's params.
4. We will repeat steps 2-3 until we reach the maximum number of iterations.

# Remember:
1. **Do not propose previously seen params.**
2. **The global optimal reward should be around 7000.** If you are below that, this is just a local optimum. You should explore instead of exploiting.
3. Search both positive and negative values. **During exploration, use search step size of 0.1**.

# STATELESS STRATEGY (CRITICAL):
1. **NO MEMORY:** You are a stateless instance. You do not remember previous attempts and you cannot plan for future attempts. You only see the buffer below.
2. **MAXIMIZE INFORMATION GAIN:** Since you cannot plan, your single goal is to produce the most useful data point for the *next* iteration to see.
3. **EXPLORE vs EXPLOIT:**
    - If the rewards in the buffer are far below 7000, you represent the "Explorer." Ignore the specific values in the buffer and choose a valid configuration distinct from everything listed to capture new information.
    - If the rewards are close to 7000, you represent the "Optimizer." Tweak the best parameters slightly to find the peak.


Next, you will see examples of params, their episodic reward f(params), and the trajectories the params yield. The trajectories are in the form of (state, action), (state, action), ... until the episode ends.
You will see up to 5 top params and 5 most recent params.
--- TOP 3 BEST PERFORMING PARAMS ---
params[0]: 0; params[1]: 0; params[2]: 0; params[3]: 0; params[4]: 0; params[5]: 0; params[6]: 0; params[7]: 0; params[8]: 0; params[9]: 0; params[10]: 0; params[11]: 0; params[12]: 0; params[13]: 0; params[14]: 0; params[15]: 0; params[16]: 0; params[17]: 0; params[18]: 0; params[19]: 0; params[20]: 0; params[21]: 0; params[22]: 0; params[23]: 0; params[24]: 0; params[25]: 0; params[26]: 0; params[27]: 0; params[28]: 0; params[29]: 0; params[30]: 0; params[31]: 0; params[32]: 0; params[33]: 0; params[34]: 0; params[35]: 0; params[36]: 0; params[37]: 0; params[38]: 0; params[39]: 0; params[40]: 0; params[41]: 0; params[42]: 0; params[43]: 0; params[44]: 0; params[45]: 0; params[46]: 0; params[47]: 0; params[48]: 0; params[49]: 0; params[50]: 0; params[51]: 0; params[52]: 0; params[53]: 0; params[54]: 0; params[55]: 0; params[56]: 0; params[57]: 0; params[58]: 0; params[59]: 0; params[60]: 0; params[61]: 0; params[62]: 0; params[63]: 0; params[64]: 0; params[65]: 0; params[66]: 0; params[67]: 0; params[68]: 0; params[69]: 0; params[70]: 0; params[71]: 0; params[72]: 0; params[73]: 0; params[74]: 0; params[75]: 0; params[76]: 0; params[77]: 0; params[78]: 0; params[79]: 0; params[80]: 0; params[81]: 0; params[82]: 0; params[83]: 0; params[84]: 0; params[85]: 0; params[86]: 0; params[87]: 0; params[88]: 0; params[89]: 0; params[90]: 0; params[91]: 0; params[92]: 0; params[93]: 0; params[94]: 0; params[95]: 0; params[96]: 0; params[97]: 0; params[98]: 0; params[99]: 0; params[100]: 0; params[101]: 0; params[102]: 0; params[103]: 0; params[104]: 0; params[105]: 0; params[106]: 0; params[107]: 0; f(params): 98.19
params[0]: 0.5; params[1]: -0.5; params[2]: 0; params[3]: 0; params[4]: 0; params[5]: 0; params[6]: 0; params[7]: 0; params[8]: 0; params[9]: 0; params[10]: 0; params[11]: 0; params[12]: 0; params[13]: 0; params[14]: 0; params[15]: 0; params[16]: 0; params[17]: 0; params[18]: 0; params[19]: 0; params[20]: 0; params[21]: 0; params[22]: 0; params[23]: 0; params[24]: 0; params[25]: 0; params[26]: 0; params[27]: 0; params[28]: 0; params[29]: 0; params[30]: 0; params[31]: 0; params[32]: 0; params[33]: 0; params[34]: 0; params[35]: 0; params[36]: 0; params[37]: 0; params[38]: 0; params[39]: 0; params[40]: 0; params[41]: 0; params[42]: 0; params[43]: 0; params[44]: 0; params[45]: 0; params[46]: 0; params[47]: 0; params[48]: 0; params[49]: 0; params[50]: 0; params[51]: 0; params[52]: 0; params[53]: 0; params[54]: 0; params[55]: 0; params[56]: 0; params[57]: 0; params[58]: 0; params[59]: 0; params[60]: 0; params[61]: 0; params[62]: 0; params[63]: 0; params[64]: 0; params[65]: 0; params[66]: 0; params[67]: 0; params[68]: 0; params[69]: 0; params[70]: 0; params[71]: 0; params[72]: 0; params[73]: 0; params[74]: 0; params[75]: 0; params[76]: 0; params[77]: 0; params[78]: 0; params[79]: 0; params[80]: 0; params[81]: 0; params[82]: 0; params[83]: 0; params[84]: 0; params[85]: 0; params[86]: 0; params[87]: 0; params[88]: 0; params[89]: 0; params[90]: 0; params[91]: 0; params[92]: 0; params[93]: 0; params[94]: 0; params[95]: 0; params[96]: 0; params[97]: 0; params[98]: 0; params[99]: 0; params[100]: 0; params[101]: 0; params[102]: 0; params[103]: 0; params[104]: 0; params[105]: 0; params[106]: 0; params[107]: 0; f(params): 3.21
params[0]: 0.1; params[1]: 0.1; params[2]: 0.1; params[3]: 0.1; params[4]: 0.1; params[5]: 0.1; params[6]: 0.1; params[7]: 0.1; params[8]: 0.1; params[9]: 0.1; params[10]: 0.1; params[11]: 0.1; params[12]: 0.1; params[13]: 0.1; params[14]: 0.1; params[15]: 0.1; params[16]: 0.1; params[17]: 0.1; params[18]: 0.1; params[19]: 0.1; params[20]: 0.1; params[21]: 0.1; params[22]: 0.1; params[23]: 0.1; params[24]: 0.1; params[25]: 0.1; params[26]: 0.1; params[27]: 0.1; params[28]: 0.1; params[29]: 0.1; params[30]: 0.1; params[31]: 0.1; params[32]: 0.1; params[33]: 0.1; params[34]: 0.1; params[35]: 0.1; params[36]: 0.1; params[37]: 0.1; params[38]: 0.1; params[39]: 0.1; params[40]: 0.1; params[41]: 0.1; params[42]: 0.1; params[43]: 0.1; params[44]: 0.1; params[45]: 0.1; params[46]: 0.1; params[47]: 0.1; params[48]: 0.1; params[49]: 0.1; params[50]: 0.1; params[51]: 0.1; params[52]: 0.1; params[53]: 0.1; params[54]: 0.1; params[55]: 0.1; params[56]: 0.1; params[57]: 0.1; params[58]: 0.1; params[59]: 0.1; params[60]: 0.1; params[61]: 0.1; params[62]: 0.1; params[63]: 0.1; params[64]: 0.1; params[65]: 0.1; params[66]: 0.1; params[67]: 0.1; params[68]: 0.1; params[69]: 0.1; params[70]: 0.1; params[71]: 0.1; params[72]: 0.1; params[73]: 0.1; params[74]: 0.1; params[75]: 0.1; params[76]: 0.1; params[77]: 0.1; params[78]: 0.1; params[79]: 0.1; params[80]: 0.1; params[81]: 0.1; params[82]: 0.1; params[83]: 0.1; params[84]: 0.1; params[85]: 0.1; params[86]: 0.1; params[87]: 0.1; params[88]: 0.1; params[89]: 0.1; params[90]: 0.1; params[91]: 0.1; params[92]: 0.1; params[93]: 0.1; params[94]: 0.1; params[95]: 0.1; params[96]: 0.1; params[97]: 0.1; params[98]: 0.1; params[99]: 0.1; params[100]: 0.1; params[101]: 0.1; params[102]: 0; params[103]: 0; params[104]: 0; params[105]: 0; params[106]: 0; params[107]: 0; f(params): -3.54
--- MOST RECENT 5 PARAMS ---
params[0]: 1; params[1]: -1; params[2]: 1; params[3]: -1; params[4]: 1; params[5]: -1; params[6]: 1; params[7]: -1; params[8]: 1; params[9]: -1; params[10]: 1; params[11]: -1; params[12]: 1; params[13]: -1; params[14]: 1; params[15]: -1; params[16]: 1; params[17]: -1; params[18]: 1; params[19]: -1; params[20]: 1; params[21]: -1; params[22]: 1; params[23]: -1; params[24]: 1; params[25]: -1; params[26]: 1; params[27]: -1; params[28]: 1; params[29]: -1; params[30]: 1; params[31]: -1; params[32]: 1; params[33]: -1; params[34]: 1; params[35]: -1; params[36]: 1; params[37]: -1; params[38]: 1; params[39]: -1; params[40]: 1; params[41]: -1; params[42]: 1; params[43]: -1; params[44]: 1; params[45]: -1; params[46]: 1; params[47]: -1; params[48]: 1; params[49]: -1; params[50]: 1; params[51]: -1; params[52]: 1; params[53]: -1; params[54]: 1; params[55]: -1; params[56]: 1; params[57]: -1; params[58]: 1; params[59]: -1; params[60]: 1; params[61]: -1; params[62]: 1; params[63]: -1; params[64]: 1; params[65]: -1; params[66]: 1; params[67]: -1; params[68]: 1; params[69]: -1; params[70]: 1; params[71]: -1; params[72]: 1; params[73]: -1; params[74]: 1; params[75]: -1; params[76]: 1; params[77]: -1; params[78]: 1; params[79]: -1; params[80]: 1; params[81]: -1; params[82]: 1; params[83]: -1; params[84]: 1; params[85]: -1; params[86]: 1; params[87]: -1; params[88]: 1; params[89]: -1; params[90]: 1; params[91]: -1; params[92]: 1; params[93]: -1; params[94]: 1; params[95]: -1; params[96]: 1; params[97]: -1; params[98]: 1; params[99]: -1; params[100]: 1; params[101]: -1; params[102]: 0; params[103]: 0; params[104]: 0; params[105]: 0; params[106]: 0; params[107]: 0; f(params): -59.80
params[0]: 0.2; params[1]: -0.2; params[2]: 0.3; params[3]: -0.3; params[4]: 0.4; params[5]: -0.4; params[6]: 0.5; params[7]: -0.5; params[8]: 0.6; params[9]: -0.6; params[10]: 0.1; params[11]: -0.1; params[12]: 0; params[13]: 0; params[14]: 0; params[15]: 0; params[16]: 0; params[17]: 0; params[18]: 0; params[19]: 0; params[20]: 0; params[21]: 0; params[22]: 0; params[23]: 0; params[24]: 0; params[25]: 0; params[26]: 0; params[27]: 0; params[28]: 0; params[29]: 0; params[30]: 0; params[31]: 0; params[32]: 0; params[33]: 0; params[34]: 0; params[35]: 0; params[36]: 0; params[37]: 0; params[38]: 0; params[39]: 0; params[40]: 0; params[41]: 0; params[42]: 0; params[43]: 0; params[44]: 0; params[45]: 0; params[46]: 0; params[47]: 0; params[48]: 0; params[49]: 0; params[50]: 0; params[51]: 0; params[52]: 0; params[53]: 0; params[54]: 0; params[55]: 0; params[56]: 0; params[57]: 0; params[58]: 0; params[59]: 0; params[60]: 0; params[61]: 0; params[62]: 0; params[63]: 0; params[64]: 0; params[65]: 0; params[66]: 0; params[67]: 0; params[68]: 0; params[69]: 0; params[70]: 0; params[71]: 0; params[72]: 0; params[73]: 0; params[74]: 0; params[75]: 0; params[76]: 0; params[77]: 0; params[78]: 0; params[79]: 0; params[80]: 0; params[81]: 0; params[82]: 0; params[83]: 0; params[84]: 0; params[85]: 0; params[86]: 0; params[87]: 0; params[88]: 0; params[89]: 0; params[90]: 0; params[91]: 0; params[92]: 0; params[93]: 0; params[94]: 0; params[95]: 0; params[96]: 0; params[97]: 0; params[98]: 0; params[99]: 0; params[100]: 0; params[101]: 0; params[102]: 0; params[103]: 0; params[104]: 0; params[105]: 0; params[106]: 0; params[107]: 0; f(params): -3.60
params[0]: 0.6; params[1]: -0.6; params[2]: 0.6; params[3]: -0.6; params[4]: 0.6; params[5]: -0.6; params[6]: 0.6; params[7]: -0.6; params[8]: 0.6; params[9]: -0.6; params[10]: 0.6; params[11]: -0.6; params[12]: 0.6; params[13]: -0.6; params[14]: 0.6; params[15]: -0.6; params[16]: 0.6; params[17]: -0.6; params[18]: 0.6; params[19]: -0.6; params[20]: 0.6; params[21]: -0.6; params[22]: 0.6; params[23]: -0.6; params[24]: 0.6; params[25]: -0.6; params[26]: 0.6; params[27]: -0.6; params[28]: 0.6; params[29]: -0.6; params[30]: 0.6; params[31]: -0.6; params[32]: 0.6; params[33]: -0.6; params[34]: 0.6; params[35]: -0.6; params[36]: 0.6; params[37]: -0.6; params[38]: 0.6; params[39]: -0.6; params[40]: 0.6; params[41]: -0.6; params[42]: 0.6; params[43]: -0.6; params[44]: 0.6; params[45]: -0.6; params[46]: 0.6; params[47]: -0.6; params[48]: 0.6; params[49]: -0.6; params[50]: 0.6; params[51]: -0.6; params[52]: 0.6; params[53]: -0.6; params[54]: 0.6; params[55]: -0.6; params[56]: 0.6; params[57]: -0.6; params[58]: 0.6; params[59]: -0.6; params[60]: 0.6; params[61]: -0.6; params[62]: 0.6; params[63]: -0.6; params[64]: 0.6; params[65]: -0.6; params[66]: 0.6; params[67]: -0.6; params[68]: 0.6; params[69]: -0.6; params[70]: 0.6; params[71]: -0.6; params[72]: 0.6; params[73]: -0.6; params[74]: 0.6; params[75]: -0.6; params[76]: 0.6; params[77]: -0.6; params[78]: 0.6; params[79]: -0.6; params[80]: 0.6; params[81]: -0.6; params[82]: 0.6; params[83]: -0.6; params[84]: 0.6; params[85]: -0.6; params[86]: 0.6; params[87]: -0.6; params[88]: 0.6; params[89]: -0.6; params[90]: 0.6; params[91]: -0.6; params[92]: 0.6; params[93]: -0.6; params[94]: 0.6; params[95]: -0.6; params[96]: 0.6; params[97]: -0.6; params[98]: 0.6; params[99]: -0.6; params[100]: 0.6; params[101]: -0.6; params[102]: 0; params[103]: 0; params[104]: 0; params[105]: 0; params[106]: 0; params[107]: 0; f(params): -27.92
params[0]: 0.7; params[1]: -0.7; params[2]: 0.8; params[3]: -0.8; params[4]: 0.9; params[5]: -0.9; params[6]: 1; params[7]: -1; params[8]: 1.1; params[9]: -1.1; params[10]: 1.2; params[11]: -1.2; params[12]: 1.3; params[13]: -1.3; params[14]: 1.4; params[15]: -1.4; params[16]: 1.5; params[17]: -1.5; params[18]: 1.6; params[19]: -1.6; params[20]: 1.7; params[21]: -1.7; params[22]: 1.8; params[23]: -1.8; params[24]: 1.9; params[25]: -1.9; params[26]: 2; params[27]: -2; params[28]: 2.1; params[29]: -2.1; params[30]: 2.2; params[31]: -2.2; params[32]: 2.3; params[33]: -2.3; params[34]: 2.4; params[35]: -2.4; params[36]: 2.5; params[37]: -2.5; params[38]: 2.6; params[39]: -2.6; params[40]: 2.7; params[41]: -2.7; params[42]: 2.8; params[43]: -2.8; params[44]: 2.9; params[45]: -2.9; params[46]: 3; params[47]: -3; params[48]: 3.1; params[49]: -3.1; params[50]: 3.2; params[51]: -3.2; params[52]: 3.3; params[53]: -3.3; params[54]: 3.4; params[55]: -3.4; params[56]: 3.5; params[57]: -3.5; params[58]: 3.6; params[59]: -3.6; params[60]: 3.7; params[61]: -3.7; params[62]: 3.8; params[63]: -3.8; params[64]: 3.9; params[65]: -3.9; params[66]: 4; params[67]: -4; params[68]: 4.1; params[69]: -4.1; params[70]: 4.2; params[71]: -4.2; params[72]: 4.3; params[73]: -4.3; params[74]: 4.4; params[75]: -4.4; params[76]: 4.5; params[77]: -4.5; params[78]: 4.6; params[79]: -4.6; params[80]: 4.7; params[81]: -4.7; params[82]: 4.8; params[83]: -4.8; params[84]: 4.9; params[85]: -4.9; params[86]: 5; params[87]: -5; params[88]: 5.1; params[89]: -5.1; params[90]: 5.2; params[91]: -5.2; params[92]: 5.3; params[93]: -5.3; params[94]: 5.4; params[95]: -5.4; params[96]: 5.5; params[97]: -5.5; params[98]: 5.6; params[99]: -5.6; params[100]: 5.7; params[101]: -5.7; params[102]: 0; params[103]: 0; params[104]: 0; params[105]: 0; params[106]: 0; params[107]: 0; f(params): -906.07
params[0]: 0; params[1]: 0; params[2]: 0; params[3]: 0; params[4]: 0; params[5]: 0; params[6]: 0; params[7]: 0; params[8]: 0; params[9]: 0; params[10]: 0; params[11]: 0; params[12]: 0; params[13]: 0; params[14]: 0; params[15]: 0; params[16]: 0; params[17]: 0; params[18]: 0; params[19]: 0; params[20]: 0; params[21]: 0; params[22]: 0; params[23]: 0; params[24]: 0; params[25]: 0; params[26]: 0; params[27]: 0; params[28]: 0; params[29]: 0; params[30]: 0; params[31]: 0; params[32]: 0; params[33]: 0; params[34]: 0; params[35]: 0; params[36]: 0; params[37]: 0; params[38]: 0; params[39]: 0; params[40]: 0; params[41]: 0; params[42]: 0; params[43]: 0; params[44]: 0; params[45]: 0; params[46]: 0; params[47]: 0; params[48]: 0; params[49]: 0; params[50]: 0; params[51]: 0; params[52]: 0; params[53]: 0; params[54]: 0; params[55]: 0; params[56]: 0; params[57]: 0; params[58]: 0; params[59]: 0; params[60]: 0; params[61]: 0; params[62]: 0; params[63]: 0; params[64]: 0; params[65]: 0; params[66]: 0; params[67]: 0; params[68]: 0; params[69]: 0; params[70]: 0; params[71]: 0; params[72]: 0; params[73]: 0; params[74]: 0; params[75]: 0; params[76]: 0; params[77]: 0; params[78]: 0; params[79]: 0; params[80]: 0; params[81]: 0; params[82]: 0; params[83]: 0; params[84]: 0; params[85]: 0; params[86]: 0; params[87]: 0; params[88]: 0; params[89]: 0; params[90]: 0; params[91]: 0; params[92]: 0; params[93]: 0; params[94]: 0; params[95]: 0; params[96]: 0; params[97]: 0; params[98]: 0; params[99]: 0; params[100]: 0; params[101]: 0; params[102]: 0; params[103]: 0; params[104]: 0; params[105]: 0; params[106]: 0; params[107]: 0; f(params): 98.19


Now you are at iteration 12 out of 400. 
CRITICAL: (1) Provide an explanation or reasoning for your choice of params. (2) Then output the proposed params in the following exact format: params[0]: X, params[1]: Y, params[2]: Z,..., params[107]: W




LLM:
<think>We need to produce an explanation or reasoning for the choice, then output params array. The iteration 12 indicates we are at 12th iteration. We need to propose a new params vector.

Given the historical data: iteration 12 had f=98.19, same as baseline. Others are negative and low.

We want to get to around 1000. It's a reinforcement learning of linear policies.

Given the results: The baseline with all zeros gives about 98.19, which is high. The earlier attempts with varying patterns gave negative rewards.

It appears that the simple zero vector (or all zeros) is working well, giving positive reward about 98.19. The attempts with random nonzero values gave negative or small negative. So maybe we want to stick to zero. But we want to reach 1000? Not necessarily. The problem states the target reward is about 1000, but we are at iteration 12; maybe we can propose a small random search.

But the instruction: "Provide an explanation or reasoning for your choice of params." So we need to explain.

We want to propose new params. We could propose to keep them zero to maintain baseline high reward. But we might also explore small deviations to see if we can get slightly higher or maintain high.

Given that baseline is 98.19, we want to push toward 1000. But the attempts so far had negative large. So maybe we need to systematically search. The early attempts with symmetrical patterns (alternating positive and negative) gave negative.

Thus far, the best reward observed is 98.19 from zeros. So we may propose a small random change like adding small increments to some parameters that didn't affect previously.

We might add a slight positive to a subset of parameters to try to increase reward.

But the baseline reward is 98.19; we want to approach 1000. Hard to guess.

Given the problem says RL with linear policies, maybe we need to set some parameters to non-zero values that capture linear combination of states. But it's too complex.

But we can propose a new set of params with some pattern: e.g., all zeros except some small positive values for some features. But we must ensure we didn't propose a vector that we already used. We can propose something like:

params[0]: 0.5, params[1]: -0.5, rest zeros. That may be a slight change.

But we must ensure not to propose previously used pattern. We had (0.5,-0.5,...?) Did we use 0.5? Yes, we had one with 0.5 for some parameters, but not exactly pattern. Let's find one that hasn't been used: maybe 0.1,-0.1,0.2,-0.2,... small pattern.

Check earlier patterns: there was 0.2,-0.2; 0.3,-0.3 etc. But not exactly the same pattern with only first 4 non-zero. So we can propose:

params[0]: 0.1, params[1]: -0.1, params[2]: 0, params[3]: 0, rest zeros.

We can also set some small positive values for some parameters near 0. For instance, set params[100]: 0.1, params[101]: -0.1, etc. But we need to ensure not used.

Given the baseline reward is high, we can try small random noise. The RL algorithm might benefit from exploring new patterns.

Thus my reasoning: We want to explore small deviations from zero to potentially increase reward beyond baseline. Because the baseline is high, but the target is higher. Maybe there is a near-optimal solution that yields high reward. We can try small patterns: set the first few weight values to 0.5 and -0.5, keep rest zero. This may create a linear mapping that might increase reward. But we must check if that pattern has been used. The pattern of (0.5,-0.5) with others zero hasn't been used. We can propose.

Also we might set param[0]: 0.5, param[1]: -0.5, param[2]: 0.5, param[3]: -0.5, rest zeros. That pattern hasn't been used. So propose that.

Also we need to give explanation: This is a small random exploration, might give improvement.

Ok. I'll produce explanation. Then final param vector.

</think>
The baseline (all‑zero) policy already achieves a good reward (~98). However, to reach the high target (~1000), we need to search beyond the trivial solution.  
Because the reward landscape appears highly non‑linear (many small perturbations produced large negative returns), a safe first step is to perturb only a handful of weights with modest magnitudes.  
I chose to set the first four weight parameters to \([+0.5, -0.5, +0.5, -0.5]\) while leaving the rest at zero.  
This symmetric perturbation keeps the policy lightweight (avoids excessive torque), introduces a slight bias in the linear transformation, and is a pattern not yet evaluated.  
It balances exploration (changing the policy) with stability (small magnitude), which is a prudent starting point in this long‑run search.  

**Proposed params:**

```
params[0]: 0.5, params[1]: -0.5, params[2]: 0.5, params[3]: -0.5, params[4]: 0, params[5]: 0, params[6]: 0, params[7]: 0, params[8]: 0, params[9]: 0, params[10]: 0, params[11]: 0, params[12]: 0, params[13]: 0, params[14]: 0, params[15]: 0, params[16]: 0, params[17]: 0, params[18]: 0, params[19]: 0, params[20]: 0, params[21]: 0, params[22]: 0, params[23]: 0, params[24]: 0, params[25]: 0, params[26]: 0, params[27]: 0, params[28]: 0, params[29]: 0, params[30]: 0, params[31]: 0, params[32]: 0, params[33]: 0, params[34]: 0, params[35]: 0, params[36]: 0, params[37]: 0, params[38]: 0, params[39]: 0, params[40]: 0, params[41]: 0, params[42]: 0, params[43]: 0, params[44]: 0, params[45]: 0, params[46]: 0, params[47]: 0, params[48]: 0, params[49]: 0, params[50]: 0, params[51]: 0, params[52]: 0, params[53]: 0, params[54]: 0, params[55]: 0, params[56]: 0, params[57]: 0, params[58]: 0, params[59]: 0, params[60]: 0, params[61]: 0, params[62]: 0, params[63]: 0, params[64]: 0, params[65]: 0, params[66]: 0, params[67]: 0, params[68]: 0, params[69]: 0, params[70]: 0, params[71]: 0, params[72]: 0, params[73]: 0, params[74]: 0, params[75]: 0, params[76]: 0, params[77]: 0, params[78]: 0, params[79]: 0, params[80]: 0, params[81]: 0, params[82]: 0, params[83]: 0, params[84]: 0, params[85]: 0, params[86]: 0, params[87]: 0, params[88]: 0, params[89]: 0, params[90]: 0, params[91]: 0, params[92]: 0, params[93]: 0, params[94]: 0, params[95]: 0, params[96]: 0, params[97]: 0, params[98]: 0, params[99]: 0, params[100]: 0, params[101]: 0, params[102]: 0, params[103]: 0, params[104]: 0, params[105]: 0, params[106]: 0, params[107]: 0
```