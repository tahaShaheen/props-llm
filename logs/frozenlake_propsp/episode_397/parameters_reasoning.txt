You are good global RL policy optimizer, helping me find the global optimal policy in the following environment:

# Environment:
The FrozenLake environment is a 2D 4x4 grid world (4 rows, 4 columns) where the agent can move left, down, right, or up. 
The state is between 0 and 15, representing the which grid the agent is in. The top row is 0 to 3, 2nd row is 4 to 7, 3rd row is 8 to 11, bottom row is 12 to 15. 
The environment is slippery, which means when the agent takes an action, it will go to 3 directions with equal probability.
- When the agent takes action 0, it will pick from [move down, move left, move up] with equal probability.
- When the agent takes action 1, it will pick from [move right, move down, move left] with equal probability.
- When the agent takes action 2, it will pick from [move up, move right, move down] with equal probability.
- When the agent takes action 3, it will pick from [move left, move up, move right] with equal probability.
The goal is to reach the state 15 (the right bottom grid), which will yield a +1 reward. All other states yield a reward of 0. However, there are some holes in the map that will end the game once the agent step onto it. You need to explore through it by reading history trajectories.
The policy is a function that maps the state to the action to take. The policy is represented by an array of 16 numbers. The value of params[i] represents the action to take when the state is i. The value of params[0] means which action to take when the agent is on the top left grid (0, 0). The value of params[1] means which action to take when the agent is on the top second grid (0, 1). So on and so forth. The agent should locate the wholes via previous explored trajectiores and pick actions that do not have possibility of falling into the wholes.

# Regarding the policy parameters **params**:
**params** is an array of 16 numbers.
**params** values should be chosen from [0, 1, 2, 3].
The value of params[i] represents the action to take when the state is i.
f(params) is the episodic reward of the policy.

# Here's how we'll interact:
1. I will first provide MAX_ITERATIONS (5000) along with a few training examples.
2. You will provide your response in the following exact format ONLY:
   * 'params[0]: X, params[1]: Y, params[2]: Z,..., params[15]: W'
   Propose params values from [0, 1, 2, 3]. OUTPUT ONLY THIS LINE, NOTHING ELSE.
3. I will then provide the policy's obtained reward f(params) averaged over 20 episodes, given the last iteration's params.
4. We will repeat steps 2-3 until we reach the maximum number of iterations.

# Remember:
1. **Do not propose previously seen params.**
2. **The global optimal reward should be around 1.0.**
3. Search all the possible values of params.

# STATELESS STRATEGY (CRITICAL):
1. **NO MEMORY:** You are a stateless instance. You do not remember previous attempts and you cannot plan for future attempts. You only see the buffer below.
2. **MAXIMIZE INFORMATION GAIN:** Since you cannot plan, your single goal is to produce the most useful data point for the *next* iteration to see.
3. **EXPLORE vs EXPLOIT:**
   - If the rewards in the buffer are far below 1.0, you represent the "Explorer." Ignore the specific values in the buffer and choose a valid configuration distinct from everything listed to capture new information.
   - If the rewards are close to 1.0, you represent the "Optimizer." Tweak the best parameters slightly to find the peak.

Next, you will see examples of params, their episodic reward f(params), and the trajectories the params yield. The trajectories are in the form of (state, action), (state, action), ... until the agent falls into a hole or gets to the goal. 
You will see up to 10 top params and 5 most recent params.
--- TOP 5 BEST PERFORMING PARAMS ---
params[0]: 2; params[1]: 3; params[2]: 1; params[3]: 3; params[4]: 2; params[5]: 0; params[6]: 1; params[7]: 2; params[8]: 3; params[9]: 1; params[10]: 0; params[11]: 2; params[12]: 3; params[13]: 1; params[14]: 3; params[15]: 2; f(params): 0.25
Trajectory: (0,1) (4,2) (8,2) 
params[0]: 1; params[1]: 1; params[2]: 1; params[3]: 0; params[4]: 1; params[5]: 1; params[6]: 1; params[7]: 0; params[8]: 1; params[9]: 1; params[10]: 1; params[11]: 0; params[12]: 1; params[13]: 1; params[14]: 1; params[15]: 0; f(params): 0.20
Trajectory: (0,1) (4,3) 
params[0]: 2; params[1]: 3; params[2]: 2; params[3]: 3; params[4]: 2; params[5]: 3; params[6]: 2; params[7]: 3; params[8]: 2; params[9]: 3; params[10]: 2; params[11]: 3; params[12]: 2; params[13]: 3; params[14]: 2; params[15]: 3; f(params): 0.20
Trajectory: (0,1) (1,0) 
params[0]: 1; params[1]: 1; params[2]: 1; params[3]: 0; params[4]: 1; params[5]: 1; params[6]: 1; params[7]: 0; params[8]: 1; params[9]: 1; params[10]: 1; params[11]: 0; params[12]: 1; params[13]: 1; params[14]: 1; params[15]: 2; f(params): 0.20
Trajectory: (0,2) (1,0) (1,0) 
params[0]: 1; params[1]: 3; params[2]: 1; params[3]: 3; params[4]: 1; params[5]: 3; params[6]: 1; params[7]: 3; params[8]: 1; params[9]: 3; params[10]: 1; params[11]: 3; params[12]: 1; params[13]: 3; params[14]: 1; params[15]: 3; f(params): 0.15
Trajectory: (0,3) (0,3) (0,3) (0,3) (1,2) (1,2) (1,2) (1,2) 
--- MOST RECENT 4 PARAMS ---
params[0]: 3; params[1]: 2; params[2]: 1; params[3]: 0; params[4]: 3; params[5]: 2; params[6]: 1; params[7]: 0; params[8]: 3; params[9]: 2; params[10]: 1; params[11]: 0; params[12]: 3; params[13]: 2; params[14]: 1; params[15]: 0; f(params): 0.05
Trajectory: (0,3) (1,3) (1,3) (1,3) (2,3) (2,3) (1,3) (2,3) (2,3) (3,0) 
params[0]: 3; params[1]: 2; params[2]: 3; params[3]: 1; params[4]: 3; params[5]: 2; params[6]: 3; params[7]: 0; params[8]: 3; params[9]: 2; params[10]: 3; params[11]: 1; params[12]: 3; params[13]: 2; params[14]: 3; params[15]: 0; f(params): 0.00
Trajectory: (0,0) (0,0) (0,0) (0,0) (0,0) (0,0) (0,0) (0,0) (4,0) (0,0) (0,0) (4,0) (4,0) (4,0) (8,3) (4,0) (0,0) (4,0) (0,0) (0,0) (4,0) (0,0) (0,0) (0,0) (0,0) (4,0) (8,3) (4,0) (4,0) (8,3) (9,3) (10,1) 
params[0]: 3; params[1]: 1; params[2]: 0; params[3]: 2; params[4]: 1; params[5]: 3; params[6]: 2; params[7]: 0; params[8]: 3; params[9]: 1; params[10]: 0; params[11]: 2; params[12]: 1; params[13]: 3; params[14]: 2; params[15]: 0; f(params): 0.00
Trajectory: (0,0) (4,1) (8,1) (8,1) (8,1) 
params[0]: 1; params[1]: 1; params[2]: 1; params[3]: 0; params[4]: 1; params[5]: 1; params[6]: 1; params[7]: 0; params[8]: 1; params[9]: 1; params[10]: 1; params[11]: 0; params[12]: 1; params[13]: 1; params[14]: 1; params[15]: 2; f(params): 0.05
Trajectory: (0,3) (0,3) (1,0) (1,0) (1,0) (1,0) (1,0) 


Now you are at iteration 397 out of 5000. OUTPUT ONLY THIS SINGLE LINE:
params[0]: X, params[1]: Y, params[2]: Z,..., params[15]: W
Do NOT output anything else. No explanation, no reasoning, nothing.



LLM:
<think>
Okay, so I'm trying to help optimize the policy for this FrozenLake environment. Let me first make sure I understand the problem correctly.

The environment is a 4x4 grid where each cell represents a state from 0 to 15. The goal is to reach state 15 (bottom-right corner) and get a reward of +1, while avoiding holes which end the episode with no reward. The movement is slippery, meaning each action has three possible outcomes with equal probability.

The policy is defined by an array 'params' where each element corresponds to the action taken in that state. My job is to find the optimal set of actions for each state (0-15) such that the average episodic reward approaches 1.0, which would indicate success.

Looking at the provided data, the top-performing policies have rewards around 0.20 to 0.25. The most recent policies are even worse, with some getting as low as 0.00 or 0.05. This suggests that we're not yet close to the optimal reward of 1.0, so according to the strategy, I should act as an "Explorer" and try new configurations distinct from previous ones.

I need to propose a 'params' array where each element is 0-3, making sure it's different from all previously seen params arrays in the buffer. My goal is to maximize information gain by exploring new territories rather than exploiting current knowledge since the rewards are still low.

Let me think about what states near the goal might look like and which actions would be best there. For example, state 15 is the goal, so any action taken there doesn't matter because the episode ends upon arrival. But for states leading to it, I should choose actions that maximize the chance of moving towards the goal.

I also need to consider the slippery nature: each action has a 1/3 chance of going in three directions. So, from each state, I want an action that, on average, moves the agent closer to the goal or away from holes.

Looking at some example trajectories:

- One trajectory went (0,1) → (4,2) → (8,2). That's moving down twice, which might be a path towards the bottom-right.
- Another had (0,3), which is the top-right corner. From there, taking action 0 leads to possible moves down, left, or up, but since it's at the edge, some moves would take it out of bounds, leading to potential falls into holes.

I should focus on ensuring that from each state, the chosen actions don't lead into known holes and have a high probability of moving towards the goal. Maybe for states near the bottom-right (like 12, 13, 14, 15), choosing right or down actions might be beneficial, but considering the slipperiness.

Perhaps starting by setting all params to action 0 first, then seeing what happens. But that's a random guess. Alternatively, I could design a policy where each state chooses the action that most often leads towards increasing row and column indices.

Wait, another approach is to look for existing policies and see which states are being handled well or poorly. For instance, in the top-performing params, state 0 takes action 2, which corresponds to trying up, right, or down. But since the agent starts at (0,0), moving up isn't possible, so it could go left (but that's a hole) or down.

Alternatively, maybe setting all states except those near the goal to move towards increasing row and column indices, while the ones closer take actions directly towards the goal.

But without knowing which states are holes, it's tricky. However, I can infer from trajectories where agents fell into holes. For example, in one trajectory, after (0,3), they took action 0 but ended up moving left or down, possibly falling into a hole.

I should make sure that actions don't lead the agent into known dangerous areas. Maybe setting all states to prefer right and down movements, as those would generally move towards the goal.

Wait, looking at the most recent params:

- One had params[0] = 3 (action 3: left, up, right). But starting from 0, action 3 could try moving right, which is better.
Another had params[0] =1 (action 1: right, down, left). So perhaps action 1 for state 0.

But I need to choose all 16 parameters. Maybe a grid-filling approach where each state's action leads towards the next state that's closer to 15.

Alternatively, for each state, pick an action that maximizes the chance of moving right or down, depending on position.

Let me try to create such a policy:

For states in rows above row 3 (0-7), prefer moving down. For states in columns left of column 3 (0-3), prefer moving right. But considering slipperiness, each action has three possible moves.

Wait, perhaps more systematically: assign for each state the best possible action based on proximity to goal, avoiding holes.

But without knowing hole positions, it's challenging. Alternatively, consider that in a standard FrozenLake, the optimal policy is to move towards the goal as much as possible. But here, due to slipperiness, it's more about getting into positions that have higher chances of moving closer.

Wait, perhaps I can look at other people's solutions or common strategies for slippery FrozenLake. But since I'm supposed to figure this out myself, let me proceed.

Let me attempt a policy where each state i chooses the action that has the highest probability of leading towards the goal. For example:

- States in row 3 (12-15): already near the bottom. So choose actions that move right or down, but since they can't go beyond, perhaps just right for column 0 and 1, and down for others.

Wait, no. State 15 is the goal; once there, done.

For state 14 (row 3, column 2), the best action would be to move right to reach 15, but considering slipperiness, any action that could take it to 15 or stay in 14 or go left. Hmm.

Alternatively, maybe set all states except those near the goal to prefer moving towards increasing row and column indices, while the closer ones just choose the direct path.

But without knowing hole positions, this is speculative. Maybe I can propose a policy where each state i chooses an action that tends to move right or down, avoiding left unless necessary, and up only if it's at the bottom.

Alternatively, perhaps all states except those in row 3 choose action 2 (which has options up, right, down). That way, from any state not in the last row, they can move up, right, or down. From row 3, maybe prefer right.

Wait, no—action 2 for non-bottom rows would allow moving up, which could lead to going back, but that might be risky if there are holes above.

Alternatively, perhaps set all states except those in the last column to choose action 1 (right, down, left), which includes a right move. But I'm not sure.

This is getting complicated. Maybe it's better to try a policy where each state i chooses an action that tries to move towards the goal as much as possible, considering all four directions but prioritizing right and down.

Alternatively, since in standard FrozenLake (non-slippery), the optimal policy is deterministic, moving directly towards the goal. But here, slipperiness adds uncertainty, so the policy needs to account for that.

Wait, perhaps I can look at some of the previous params and see what worked or didn't work.

Looking back, one of the top params had:

params[0]: 2; ... So action 2 for state 0. Action 2 is up, right, down. From state 0 (top-left), moving up isn't possible, so it could go left (hole) or down. But perhaps that's risky.

Another had params[0] =1: action 1 is right, down, left. So from state 0, this would have a chance to move right (towards the goal), which might be better.

I think starting by setting all states to prefer actions that increase their row and column indices as much as possible would be a good initial exploration.

So for each state i:

- If in row <3, choose an action that can move down or right.
- If in column <3, choose an action that can move right or down.

But considering the available actions:

Action 0: down, left, up
Action 1: right, down, left
Action 2: up, right, down
Action 3: left, up, right

So for a state in row <3 and column <3, choosing action 1 or 2 would allow moving right or down.

Alternatively, perhaps set all states to choose action 1 (right, down, left) except when near the edges where other actions might be better.

But I'm not sure. Maybe setting params[0] = 1, as that allows movement right from the start.

Let me try to create a policy where each state i chooses an action that tends to move towards increasing row and column indices:

For states in row r (0-3) and column c (0-3):

If r <3: prefer down or right
If c <3: prefer right or down

But since the actions have multiple possibilities, perhaps choosing actions that include both.

Looking at the action options:

Action 1 includes right, down, left. So for most states, this could be a good option as it allows moving right or down but also has a chance to move left, which might not be ideal.

Alternatively, Action 2: up, right, down. This allows moving up (which is bad if there's a hole above) but includes right and down.

Hmm. Maybe for states not in the last row, choose action 1 (to have a chance to go right or down), and for those in the last row, choose action 3 (to move right or up).

Wait, no—if in the last row, moving down isn't possible, so from state 12-15, actions that move right would be better.

But I'm not sure. Alternatively, set all states to action 0 except those in the bottom-right corner.

Alternatively, perhaps a uniform policy where each state chooses an action at random, but that's not helpful.

Wait, another approach: since the environment is slippery and stochastic, maybe the optimal policy is to always move towards the goal direction as much as possible. So from any state, choose the action that has the highest chance of moving closer to 15.

But without knowing the exact hole positions, it's hard to know which actions are safe.

Alternatively, perhaps set all states to choose action 2 (up, right, down). This way, they can move in multiple directions but have a higher chance of moving towards the goal.

Wait, let me try to create such a policy:

For each state i from 0-15:

params[i] = 2

But I need to make sure that this hasn't been tried before. Looking back at previous params, one had all 2s except some varying in certain states. So maybe this is a new configuration.

Alternatively, perhaps vary it slightly: set most to 2 but change a few to other actions based on their position.

But without knowing the hole positions, it's hard to decide which ones to change.

Given that I'm supposed to be an Explorer since rewards are low, I should propose a completely new params array that hasn't been seen before.

Looking at the previous params arrays:

- The top one had varying actions but mostly 1s and 3s.
- Another had all 2s except some states.

So perhaps setting most to action 0 or 1 could be novel.

Wait, another thought: in standard FrozenLake (non-slippery), the optimal policy is to move right and down. But with slipperiness, maybe a more cautious approach is needed, such as always moving towards the goal direction but allowing for some variance.

Alternatively, perhaps setting all states to action 0 except those where it's better to choose another action.

But I'm not sure. Maybe try setting params[0] =1, and others to varying actions.

Wait, let me think about each state:

- State 0 (0,0): can move down, left, or up. But moving left is a hole, so best to avoid. So perhaps action 1 (right, down, left). From state 0, right would take it to state 3, which might be better.

Wait, actually in the grid:

Each state i corresponds to (row, column) as follows:
- i=0: row 0, col 0
- i=1: row 0, col 1
- ...
- i=3: row 0, col 3
- i=4: row 1, col 0
- etc.

So from state 0 (row 0, col 0), possible moves:

Action 0: down to 4, left (out of bounds, which is a hole?), up (invalid). So action 0 might lead to falling into a hole if moving left. So perhaps action 0 is bad for state 0.

Action 1: right (to 1), down (to 4), or left (hole). So better than action 0 as it has a chance to move right.

Similarly, action 2: up (invalid), right (to 1), down (to 4).

Action 3: left (hole), up (invalid), right (to 1). So action 3 is risky because of the left option.

Thus, for state 0, actions 1 or 2 are better as they have a higher chance to move towards the goal without risking the hole on the left.

So maybe set params[0] =1 or 2.

Similarly, for other edge states:

State 3 (row 0, col 3): can't move right. So actions that avoid moving left would be better. Action 1: down (to 7), right (invalid), left (to 2). But moving left is possible but might lead away. Alternatively, action 2: up (invalid), right (invalid), down (to 7). So action 2 for state 3 could be better.

Wait, in state 3:

Action 0: down to 7, left to 2, up invalid.
Action 1: down to 7, right invalid, left to 2.
Action 2: down to 7, right invalid, up invalid.
So action 2 for state 3 would only have two options: down and right (but right is invalid). So it's better than action 0 or 1 as it doesn't allow moving left.

Thus, maybe set params[3] =2.

Similarly, for other states:

State 4 (row 1, col 0): can move up (to 0), down (to 8), right (to5). So actions that don't go up might be better to avoid state 0 if it's a hole. But without knowing, it's hard.

Alternatively, set params[4] =2 (action 2: up, right, down). That way, there's a chance to move up, right, or down.

Wait, but moving up from state 4 takes you back to state 0. If state 0 is safe, that could be okay. But if it's a hole, then bad. So perhaps set params[4] =1 (action 1: right, down, left). That way, the agent can move right or down.

This approach would require setting each state to an action that allows moving towards the goal while avoiding potentially dangerous moves like going up from row 1 or row 2.

But this is getting quite involved. Since I'm supposed to propose a params array without prior knowledge of which states are holes, perhaps it's best to choose a uniform policy where most actions encourage movement towards increasing rows and columns.

Let me try creating such a policy:

For each state i (0-15):

If the agent is in row <3 and column <3: choose action 1 (right, down, left) or 2 (up, right, down).

Wait, but this could vary. Alternatively, for all states not in the last row or column, set to action 1; for those in the last row, set to action 3.

But without knowing, it's hard. Maybe just set all states to action 0 except where that would be bad.

Alternatively, perhaps set all states to action 2 except state 0 and state 3.

Wait, I'm getting stuck here. Let me think differently: since the environment is slippery, the policy needs to account for the stochasticity. So a good approach might be to have each state choose an action that maximizes the expected reward, considering all possible transitions.

But without knowing which states are holes and where the goal is, it's difficult. Alternatively, perhaps assume that only certain states are holes, like in standard FrozenLake (e.g., [8, 14, 13] are holes). But I'm not sure if this environment follows that setup.

Assuming that some states are holes, such as state 5 and 7 in the second row, which might be holes. Then, from state 4 (row 1, col 0), choosing an action that doesn't lead to these holes would be better.

But without explicit information, I can only make educated guesses.

Perhaps it's best to propose a policy where each state i chooses action 2 except for certain edge cases. For example:

- params[i] =2 for all i except when in row 0 and column <3, set to 1 or 3.

Alternatively, perhaps set all states to 2, which allows moving up, right, down, giving a good chance of moving towards the goal without going back too much.

Wait, but looking at previous params, one had all 2s except some varying. So I need to propose something different.

Perhaps set most to 1 and vary a few. For example:

params[i] =1 for all i except states in the last row and column, which could be set to action 3.

But this is just a guess.

Alternatively, since the agent needs to reach state 15, maybe setting all states to choose actions that move towards higher-numbered states as much as possible.

So:

- For state i, if moving right or down leads to a higher-numbered state, choose actions that have those options.

But considering the action's available moves:

Action 0: down, left, up
Action 1: right, down, left
Action 2: up, right, down
Action 3: left, up, right

So for most states, choosing action 1 or 2 would allow moving towards higher-numbered states.

Thus, perhaps set all states to action 1 except when in the last row or column.

But without knowing hole positions, it's still a guess.

Given that I'm supposed to act as an Explorer and propose a new params array not seen before, I'll create one where:

- For states in row 0 (0-3): choose action 1
- For states in row 1 (4-7): choose action 2
- For states in row 2 (8-11): choose action 3
- For states in row 3 (12-15): choose action 0

But I'm not sure if this is novel. Looking back, previous params had various combinations.

Alternatively, set all to action 0 except for some strategic positions.

Wait, but that might not be the best approach.

Another idea: alternate actions in a pattern. For example:

params[even i] =1
params[odd i]=2

But I'm not sure if this has been tried before.

Looking back at previous params, one had all 2s except some varying. So perhaps try setting most to action 3 and vary others.

Alternatively, set all states to action 0, but that might have been tried already.

Wait, in the most recent params, one had params[0]=3, which is action 3: left, up, right. From state 0, this could be risky because moving left is a hole.

So perhaps setting params[0] =1 would be better.

Let me try to construct a params array where:

- For states in row 0 (0-3): choose action 1
- For states in row 1 (4-7): choose action 2
- For states in row 2 (8-11): choose action 3
- For states in row 3 (12-15): choose action 0

But I need to make sure this is a new configuration.

Alternatively, perhaps set all states to action 3 except for some edge cases. But without knowing the hole positions, it's hard.

Wait, another approach: create a policy where each state i chooses an action that tries to move towards the goal direction based on its current position.

For example:

- From any state not in the last column (c <3), choose an action that includes moving right.
- From any state not in the bottom row (r <3), choose an action that includes moving down.

Looking at the actions:

Action 1 includes right, so for non-last column states, set to 1.
Action 2 includes down, but also up and right. So for non-bottom row states, perhaps 2 is better.

But this might conflict. Alternatively, prioritize right over down in columns.

Alternatively, for all states where column <3: choose action 1 (right)
For states where column=3 and row<3: choose action 0 or 2
For states where row >=3: choose action 3 to move right if possible

But I'm not sure. Maybe this is getting too complicated.

Given the time constraints, perhaps it's best to propose a policy where each state i chooses an action that maximizes the chance of moving towards higher-numbered states (i.e., down or right). So:

- For states in row 0: choose action 1 (right, down, left) - allows moving right
- For states in rows 1 and 2: choose action 2 (up, right, down)
- For states in row 3: choose action 3 (left, up, right)

But I'm not sure if this is novel.

Wait, but looking at the previous params, one had:

params[0]:2; ... which is action 2 for state 0. So maybe to vary it, set state 0 to action 1 instead.

Thus, my proposed params would be:

For i=0: 1
i=1:3
i=2:1
i=3:2
i=4:3
i=5:1
i=6:2
i=7:3
i=8:1
i=9:0
i=10:2
i=11:1
i=12:3
i=13:2
i=14:1
i=15:0

But I need to ensure this is new. Looking back, the top params had varying actions, but perhaps this combination hasn't been tried.

Alternatively, since I'm supposed to be an Explorer and not repeat previous attempts, maybe set all even states to 1 and odd to 2.

But without knowing, it's hard. Given that, I think I'll propose a policy where each state i chooses action (i %4), ensuring some variation.

Wait, but this might lead to repetition. Alternatively, perhaps set params[i] = i%4.

But I'm not sure if this has been tried before. Looking at previous params, one had all 2s except for some, so perhaps setting to a different pattern would be novel.

Thus, I'll propose:

params[0]:1
params[1]:3
params[2]:0
params[3]:2
params[4]:3
params[5]:1
params[6]:2
params[7]:0
params[8]:3
params[9]:1
params[10]:0
params[11]:2
params[12]:3
params[13]:1
params[14]:0
params[15]:2

But I'm not sure if this is the best approach. Alternatively, since the goal is to reach state 15, perhaps set all states to action 1 except those in the last row and column.

Alternatively, given time constraints, I think I'll propose a policy where most states choose action 2, but vary some key ones like state 0 and 3 to avoid holes.
</think>

params[0]:1,params[1]:3,params[2]:0,params[3]:2,params[4]:3,params[5]:1,params[6]:2,params[7]:0,params[8]:3,params[9]:1,params[10]:0,params[11]:2,params[12]:3,params[13]:1,params[14]:0,params[15]:2

params[0]:1,params[1]:3,params[2]:0,params[3]:2,params[4]:3,params[5]:1,params[6]:2,params[7]:0,params[8]:3,params[9]:1,params[10]:0,params[11]:2,params[12]:3,params[13]:1,params[14]:0,params[15]:2